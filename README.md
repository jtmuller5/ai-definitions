This repository was created to aggregate the growing list of technical terms used in the AI and ML fields.

# Terms
| Term       | Definition     | Source  |
|------------|-----------------|--------|
| **Activation Function** | An activation function is a function that is used to determine the output of a neural network. | [Activation Function](https://en.wikipedia.org/wiki/Activation_function)|

# Abbreviations

| Term       | Definition     | Source  |
|------------|-----------------|--------|
| **BLAS**   | BLAS (Basic Linear Algebra Subprograms) is a specification that prescribes a set of low-level routines for performing common linear algebra operations such as vector addition, scalar multiplication, dot products, linear combinations, and matrix multiplication. | [Basic Linear Algebra Subprograms](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms)|
| **GGUF**   | GPT-Generated Unified Format (GGUF)   | [GGML vs GGUF](https://medium.com/@phillipgimmi/what-is-gguf-and-ggml-e364834d241c)|
| **GGML**   | Georgi Gerganovâ€™s Machine Learning (GGML) was an early attempt to create a file format for storing GPT models.    | [GGML vs GGUF](https://medium.com/@phillipgimmi/what-is-gguf-and-ggml-e364834d241c)|
| **GLU**    | GLU (Gated Linear Unit) is a gating mechanism that uses a sigmoid function to gate the output of a linear layer.  | [Language Models are Unsupervised Multitask Learners](https://arxiv.org/abs/1612.08083v3)|
| **FSDP**   | Fully Sharded Data Parallelism (FSDP) is a data parallel training technique that shards both the model and the data across multiple GPUs. | [Fully Sharded Data Parallelism in PyTorch](https://engineering.fb.com/2021/07/15/open-source/fsdp/)|
| **GPT**    | GPT (Generative Pre-trained Transformer) is a transformer-based model that uses a language model to generate text.| [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) |
| **MLP**    | MLP (Multi-Layer Perceptron) is a feedforward neural network that consists of at least three layers of nodes: an input layer, a hidden layer, and an output layer. | [A Brief Introduction to Neural Networks](https://en.wikipedia.org/wiki/Multilayer_perceptron)|
| **NLP**    | NLP (Natural Language Processing) is a field of computer science that focuses on the interaction between computers and humans using natural language.  | [Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing)|
| **RAG**    | RAG (Retrieval-Augmented Generation) is a transformer-based model that uses a retriever to retrieve relevant passages from a knowledge base and then uses a generator to generate an answer based on the retrieved passages. | [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) |
| **RoPE**   | RoPE (Rotary Positional Encoding) is a positional encoding that uses a rotary encoding scheme to represent the absolute position of a token within a sequence. | [Rotary Positional Embedding](https://arxiv.org/abs/2104.09864)   |
| **SwiGLU** | SwiGLU is an activation function which is a variant of GLU.  | [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2002.05202v1)   |
| **ZeRO**   | ZeRO (Zero Redundancy Optimizer) is a novel memory optimization technology that allows users to train deep learning models with parameter sizes that exceed GPU VRAM sizes.  | [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)|